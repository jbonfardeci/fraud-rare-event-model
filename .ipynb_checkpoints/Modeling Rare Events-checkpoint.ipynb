{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Rare Events with Fraud Detection and Random Oversampling\n",
    "John Bonfardeci | john.bonfardeci@gmail.com | 2020-04-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Mircosoft Azure ML, follow instructions here: \n",
    "    https://docs.microsoft.com/en-us/sql/machine-learning/python/setup-python-client-tools-sql?view=sql-server-ver15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from microsoftml import rx_logistic_regression, categorical, rx_predict, rx_fast_trees\n",
    "from microsoftml import rx_neural_network\n",
    "from revoscalepy import rx_import, RxXdfData, rx_summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elasticnet(X, y, column_names, max_iter=10000):\n",
    "    \"\"\"\n",
    "    Select significant variables according to Elastic Net.\n",
    "    @param X <Pandas Dataframe>\n",
    "    @param y <list>\n",
    "    @columns <list>\n",
    "    @max_iter <int>\n",
    "    @returns <list>\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_mse(alpha):\n",
    "        model = ElasticNet(alpha=a, max_iter=max_iter).fit(X, y)   \n",
    "        score = model.score(X, y)\n",
    "        pred_y = model.predict(X)\n",
    "        return mean_squared_error(y, pred_y)\n",
    "        \n",
    "    lowest_mse = 1.0\n",
    "    best_alpha = 0.0\n",
    "    alphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "    \n",
    "    for a in alphas:\n",
    "        mse = get_mse(a)\n",
    "        if mse < lowest_mse:\n",
    "            lowest_mse = mse\n",
    "            best_alpha = a\n",
    "        \n",
    "    clf = ElasticNet(alpha=best_alpha, max_iter=max_iter)\n",
    "    sfm = SelectFromModel(clf)\n",
    "    sfm.fit(X, y)\n",
    "    feature_indices = sfm.get_support()\n",
    "    significant_features = []\n",
    "    for c, b in zip(column_names, feature_indices):\n",
    "        if b:\n",
    "            significant_features.append(c)\n",
    "\n",
    "    return significant_features\n",
    "\n",
    "def get_balanced_accuracy(tpr, fpr):\n",
    "    \"\"\"\n",
    "    Return average of Sensitivity and Specificity.\n",
    "    \"\"\"\n",
    "    return (tpr + (1-fpr)) / 2\n",
    "\n",
    "def get_tpr_fpr(cm):\n",
    "    \"\"\"\n",
    "    Sensitivity: TruePos / (True Pos + False Neg) \n",
    "    Specificity: True Neg / (False Pos + True Neg)\n",
    "    TN | FP\n",
    "    -------\n",
    "    FN | TP\n",
    "    @param 2D array <list<list>>\n",
    "    @returns <list<float>>\n",
    "    \"\"\"\n",
    "\n",
    "    tn = float(cm[0][0])\n",
    "    fp = float(cm[0][1])\n",
    "    fn = float(cm[1][0])\n",
    "    tp = float(cm[1][1])\n",
    "\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = 1-(tn / (fp + tn))\n",
    "\n",
    "    return [tpr, fpr] \n",
    "    \n",
    "def get_confusion_matrix(cutoff, actual, prob):\n",
    "        \"\"\"\n",
    "        Return a confusion matrix with the optimal threshold/cutoff for probability of Y.\n",
    "\n",
    "        TN | FP\n",
    "        -------\n",
    "        FN | TP\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        for (x, y) in prob:\n",
    "            pred.append(1 if y >= cutoff else 0)\n",
    "\n",
    "        return confusion_matrix(actual, pred)\n",
    "    \n",
    "def get_best_cutoff(actual, prob):  \n",
    "        \"\"\"\n",
    "        Get the best cutoff according to Balanced Accuracy\n",
    "        'Brute-force' technique - try all cutoffs from 0.01 to 0.99 in increments of 0.01\n",
    "\n",
    "        @param actual <list<float>>\n",
    "        @param prob <list<tuple<float, float>>>\n",
    "        @returns <list<float>>\n",
    "        \"\"\"\n",
    "        if not isinstance(actual, np.ndarray):\n",
    "            print(\"'actual' is not of type ndarray!\")\n",
    "            return None\n",
    "        \n",
    "        if not isinstance(prob, np.ndarray):\n",
    "            print(\"'prob' is not of type ndarray!\")\n",
    "            return None\n",
    "        \n",
    "        best_tpr = 0.0; best_fpr = 0.0; best_cutoff = 0.0; best_ba = 0.0; \n",
    "        cutoff = 0.0\n",
    "        cm = [[0,0],[0,0]]\n",
    "        while cutoff < 1.0:\n",
    "            _cm = get_confusion_matrix(cutoff=cutoff, actual=actual, prob=prob)\n",
    "            _tpr, _fpr = get_tpr_fpr(_cm)\n",
    "\n",
    "            if(_tpr < 1.0):    \n",
    "                ba = get_balanced_accuracy(tpr=_tpr, fpr=_fpr)\n",
    "\n",
    "                if(ba > best_ba):\n",
    "                    best_ba = ba\n",
    "                    best_cutoff = cutoff\n",
    "                    best_tpr = _tpr\n",
    "                    best_fpr = _fpr\n",
    "                    cm = _cm\n",
    "\n",
    "            cutoff += 0.01\n",
    "\n",
    "        tn = cm[0][0]; fp = cm[0][1]; fn = cm[1][0]; tp = cm[1][1];\n",
    "        return [best_tpr, best_fpr, best_cutoff, tn, fp, fn, tp]\n",
    "    \n",
    "# create confusion matrix\n",
    "def get_predict_frame(source_out_df, target, calc_cutoff=True):\n",
    "    \"\"\"\n",
    "    Compute predicted based on estimated probabilities and best threshold. \n",
    "    Output predictions and confusion matrix.\n",
    "    \"\"\"\n",
    "    # create dataframe\n",
    "    actual = source_out_df[target]\n",
    "    est = source_out_df[\"PredictedLabel\"]\n",
    "    dat = pd.concat([actual, est], 1)\n",
    "    dat.columns = ['Y','Yhat']\n",
    "    stats = None\n",
    "    \n",
    "    if calc_cutoff:\n",
    "        score = source_out_df['Score.1']\n",
    "        prob = source_out_df['Probability.1']\n",
    "        dat = pd.concat([dat, score, prob], 1)\n",
    "        dat.columns = ['Y','Yhat', 'Score' ,'Prob']\n",
    "        # calculate TPR, FPR, best probability threshold\n",
    "        tpr, fpr, cutoff, tn, fp, fn, tp = get_best_cutoff(actual.values, dat[['Score', 'Prob']].values)\n",
    "        #print(\"Optimal prob. threshold is %0.3f: \" % cutoff)\n",
    "        vals = dat[dat.columns].values.tolist()\n",
    "        dat['Yhat'] = list(map(lambda y: 1 if y[3] >= cutoff else 0, vals))\n",
    "        stats = pd.DataFrame(columns=['TP', 'FP', 'TN', 'FN', 'Sensitivity', 'Specificity', 'Cutoff'],\n",
    "                    data=[[tp, fp, tn, fn, tpr, (1-fpr), cutoff]])\n",
    "    else:\n",
    "        vals = dat[dat.columns].values.tolist()\n",
    "        dat['TP'] = list(map(lambda y: 1 if y[0] == y[1] and y[0] == True else 0, vals))\n",
    "        dat['TN'] = list(map(lambda y: 1 if y[0] == y[1] and y[0] == False else 0, vals))\n",
    "        dat['FP'] = list(map(lambda y: 1 if y[0] != y[1] and y[0] == False else 0, vals))\n",
    "        dat['FN'] = list(map(lambda y: 1 if y[0] != y[1] and y[0] == True else 0, vals))\n",
    "        tp = sum(dat['TP'])\n",
    "        tn = sum(dat['TN'])\n",
    "        fp = sum(dat['FP'])\n",
    "        fn = sum(dat['FN'])\n",
    "        tpr = tp/(tp+fn)\n",
    "        tnr = tn/(tn+fp)\n",
    "        stats = pd.DataFrame(columns=['TP', 'FP', 'TN', 'FN', 'Sensitivity', 'Specificity'],\n",
    "                    data=[[tp, fp, tn, fn, tpr, tnr]])\n",
    "    \n",
    "    print(\"Sensitivity: {0:.3f}%, Specificity: {1:.3f}%, Threshold: {2:.3f}\".format(tpr*100, (1-fpr)*100, cutoff))\n",
    "    return dat, stats\n",
    "\n",
    "def get_roc(actual, predicted, title):\n",
    "    \"\"\"\n",
    "    Plot ROC curve based on actuals and estimated probabilities.\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = metrics.roc_curve(actual, predicted)    \n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title(title)\n",
    "    lbl = 'AUC: {0:.3f}'.format(roc_auc)\n",
    "    plt.plot(fpr, tpr, 'b', label = lbl)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Credit Card Fraud Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 284807, Total Rows Processed: 284807, Total Chunk Time: 3.645 seconds \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import XDF data file and describe data.\n",
    "df = rx_import('creditcard.xdf')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET = 'Class' # The name of our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of fraud to non-fraud transactions is 0.173% to 99.827%.\n"
     ]
    }
   ],
   "source": [
    "# Is this a rare event problem where the minority class < 2%.\n",
    "fraud = [row for row in df.values if row[-1] == 1]\n",
    "nonfraud = [row for row in df.values if row[-1] == 0]\n",
    "ratio = len(fraud) / len(df)*100\n",
    "print(str.format(\"The ratio of fraud to non-fraud transactions is {0:.3f}% to {1:.3f}%.\", ratio, 100-ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['Time', 'Class'], 1)\n",
    "target = df['Class']\n",
    "column_names = features.columns\n",
    "\n",
    "# split into 90/10 train/validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(features.values, target.values, test_size=0.1, random_state=123)\n",
    " \n",
    "# Oversample minority class\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training Dataframe\n",
    "df_train = pd.DataFrame(data=x_train)\n",
    "df_train.columns = column_names\n",
    "df_train[TARGET] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Validation Dataframe\n",
    "df_val = pd.DataFrame(data=x_val)\n",
    "df_val.columns = column_names\n",
    "df_val[TARGET] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + Amount'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create formula string\n",
    "formula = TARGET + \" ~ \" + \" + \".join(features.columns)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Class</td>      <th>  No. Observations:  </th>  <td>256326</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>256296</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>    29</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th>    <td>1.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -1013.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Fri, 10 Apr 2020</td> <th>  Deviance:          </th> <td>  2026.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>20:32:23</td>     <th>  Pearson chi2:      </th> <td>6.67e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>        <td>12</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   -8.7147</td> <td>    0.159</td> <td>  -54.863</td> <td> 0.000</td> <td>   -9.026</td> <td>   -8.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>        <td>    0.0578</td> <td>    0.044</td> <td>    1.325</td> <td> 0.185</td> <td>   -0.028</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>        <td>    0.0356</td> <td>    0.061</td> <td>    0.583</td> <td> 0.560</td> <td>   -0.084</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>        <td>    0.0246</td> <td>    0.047</td> <td>    0.522</td> <td> 0.602</td> <td>   -0.068</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>        <td>    0.6899</td> <td>    0.078</td> <td>    8.809</td> <td> 0.000</td> <td>    0.536</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>        <td>    0.1306</td> <td>    0.067</td> <td>    1.940</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>        <td>   -0.1124</td> <td>    0.076</td> <td>   -1.473</td> <td> 0.141</td> <td>   -0.262</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>        <td>   -0.0922</td> <td>    0.069</td> <td>   -1.344</td> <td> 0.179</td> <td>   -0.227</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>        <td>   -0.1592</td> <td>    0.032</td> <td>   -5.024</td> <td> 0.000</td> <td>   -0.221</td> <td>   -0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>        <td>   -0.2682</td> <td>    0.116</td> <td>   -2.309</td> <td> 0.021</td> <td>   -0.496</td> <td>   -0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>       <td>   -0.7947</td> <td>    0.100</td> <td>   -7.917</td> <td> 0.000</td> <td>   -0.992</td> <td>   -0.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>       <td>   -0.0147</td> <td>    0.080</td> <td>   -0.185</td> <td> 0.853</td> <td>   -0.171</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>       <td>    0.0824</td> <td>    0.092</td> <td>    0.893</td> <td> 0.372</td> <td>   -0.098</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>       <td>   -0.3635</td> <td>    0.086</td> <td>   -4.249</td> <td> 0.000</td> <td>   -0.531</td> <td>   -0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>       <td>   -0.5306</td> <td>    0.065</td> <td>   -8.148</td> <td> 0.000</td> <td>   -0.658</td> <td>   -0.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>       <td>   -0.0489</td> <td>    0.088</td> <td>   -0.553</td> <td> 0.580</td> <td>   -0.222</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>       <td>   -0.2344</td> <td>    0.129</td> <td>   -1.811</td> <td> 0.070</td> <td>   -0.488</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>       <td>   -0.0259</td> <td>    0.073</td> <td>   -0.354</td> <td> 0.724</td> <td>   -0.170</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>       <td>    0.0049</td> <td>    0.133</td> <td>    0.037</td> <td> 0.971</td> <td>   -0.257</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>       <td>    0.0429</td> <td>    0.100</td> <td>    0.428</td> <td> 0.669</td> <td>   -0.153</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>       <td>   -0.4568</td> <td>    0.084</td> <td>   -5.449</td> <td> 0.000</td> <td>   -0.621</td> <td>   -0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>       <td>    0.3729</td> <td>    0.060</td> <td>    6.169</td> <td> 0.000</td> <td>    0.254</td> <td>    0.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>       <td>    0.5944</td> <td>    0.135</td> <td>    4.417</td> <td> 0.000</td> <td>    0.331</td> <td>    0.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>       <td>   -0.0730</td> <td>    0.060</td> <td>   -1.224</td> <td> 0.221</td> <td>   -0.190</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>       <td>    0.1383</td> <td>    0.154</td> <td>    0.897</td> <td> 0.370</td> <td>   -0.164</td> <td>    0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>       <td>   -0.1350</td> <td>    0.135</td> <td>   -1.003</td> <td> 0.316</td> <td>   -0.399</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>       <td>    0.1026</td> <td>    0.196</td> <td>    0.522</td> <td> 0.601</td> <td>   -0.282</td> <td>    0.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>       <td>   -0.7809</td> <td>    0.125</td> <td>   -6.231</td> <td> 0.000</td> <td>   -1.027</td> <td>   -0.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>       <td>   -0.2433</td> <td>    0.092</td> <td>   -2.631</td> <td> 0.009</td> <td>   -0.425</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th>    <td>    0.0009</td> <td>    0.000</td> <td>    2.460</td> <td> 0.014</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               256326\n",
       "Model:                            GLM   Df Residuals:                   256296\n",
       "Model Family:                Binomial   Df Model:                           29\n",
       "Link Function:                  logit   Scale:                             1.0\n",
       "Method:                          IRLS   Log-Likelihood:                -1013.3\n",
       "Date:                Fri, 10 Apr 2020   Deviance:                       2026.6\n",
       "Time:                        20:32:23   Pearson chi2:                 6.67e+05\n",
       "No. Iterations:                    12                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     -8.7147      0.159    -54.863      0.000      -9.026      -8.403\n",
       "V1             0.0578      0.044      1.325      0.185      -0.028       0.143\n",
       "V2             0.0356      0.061      0.583      0.560      -0.084       0.155\n",
       "V3             0.0246      0.047      0.522      0.602      -0.068       0.117\n",
       "V4             0.6899      0.078      8.809      0.000       0.536       0.843\n",
       "V5             0.1306      0.067      1.940      0.052      -0.001       0.263\n",
       "V6            -0.1124      0.076     -1.473      0.141      -0.262       0.037\n",
       "V7            -0.0922      0.069     -1.344      0.179      -0.227       0.042\n",
       "V8            -0.1592      0.032     -5.024      0.000      -0.221      -0.097\n",
       "V9            -0.2682      0.116     -2.309      0.021      -0.496      -0.041\n",
       "V10           -0.7947      0.100     -7.917      0.000      -0.992      -0.598\n",
       "V11           -0.0147      0.080     -0.185      0.853      -0.171       0.142\n",
       "V12            0.0824      0.092      0.893      0.372      -0.098       0.263\n",
       "V13           -0.3635      0.086     -4.249      0.000      -0.531      -0.196\n",
       "V14           -0.5306      0.065     -8.148      0.000      -0.658      -0.403\n",
       "V15           -0.0489      0.088     -0.553      0.580      -0.222       0.124\n",
       "V16           -0.2344      0.129     -1.811      0.070      -0.488       0.019\n",
       "V17           -0.0259      0.073     -0.354      0.724      -0.170       0.118\n",
       "V18            0.0049      0.133      0.037      0.971      -0.257       0.267\n",
       "V19            0.0429      0.100      0.428      0.669      -0.153       0.239\n",
       "V20           -0.4568      0.084     -5.449      0.000      -0.621      -0.293\n",
       "V21            0.3729      0.060      6.169      0.000       0.254       0.491\n",
       "V22            0.5944      0.135      4.417      0.000       0.331       0.858\n",
       "V23           -0.0730      0.060     -1.224      0.221      -0.190       0.044\n",
       "V24            0.1383      0.154      0.897      0.370      -0.164       0.441\n",
       "V25           -0.1350      0.135     -1.003      0.316      -0.399       0.129\n",
       "V26            0.1026      0.196      0.522      0.601      -0.282       0.488\n",
       "V27           -0.7809      0.125     -6.231      0.000      -1.027      -0.535\n",
       "V28           -0.2433      0.092     -2.631      0.009      -0.425      -0.062\n",
       "Amount         0.0009      0.000      2.460      0.014       0.000       0.002\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show P-values and coefficients\n",
    "stats = smf.glm(formula=formula, data=df_train, family=sm.families.Binomial()).fit()\n",
    "stats.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing data.\n",
      "Rows Read: 199364, Read Time: 0, Transform Time: 0\n",
      "Beginning processing data.\n",
      "Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.\n",
      "Beginning processing data.\n",
      "Rows Read: 199364, Read Time: 0.001, Transform Time: 0\n",
      "Beginning processing data.\n",
      "Warning: The number of threads specified in trainer arguments is larger than the concurrency factor setting of the environment. Using 2 training threads instead.\n",
      "LBFGS multi-threading will attempt to load dataset into memory. In case of out-of-memory issues, turn off multi-threading by setting trainThreads to 1.\n",
      "Beginning processing data.\n",
      "Rows Read: 199364, Read Time: 0.001, Transform Time: 0\n",
      "Beginning processing data.\n",
      "Beginning optimization\n",
      "num vars: 30\n",
      "improvement criterion: Mean Improvement\n",
      "L1 regularization selected 19 of 30 weights.\n",
      "Not training a calibrator because it is not needed.\n",
      "Elapsed time: 00:00:01.1263354\n",
      "Elapsed time: 00:00:00.0295897\n"
     ]
    }
   ],
   "source": [
    "# Fit Logistic Regression Model\n",
    "logit = rx_logistic_regression(formula, data=df_train, method='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Bias)</td>\n",
       "      <td>-7.678820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V14</td>\n",
       "      <td>-9.342113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V4</td>\n",
       "      <td>5.701827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V12</td>\n",
       "      <td>-3.756634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V11</td>\n",
       "      <td>3.035359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1\n",
       "0  (Bias) -7.678820\n",
       "1     V14 -9.342113\n",
       "2      V4  5.701827\n",
       "3     V12 -3.756634\n",
       "4     V11  3.035359"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show selected predictors and coefficients from logit model\n",
    "df_coef = pd.DataFrame(list(logit.coef_.items()))\n",
    "df_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing data.\n",
      "Rows Read: 28481, Read Time: 0.001, Transform Time: 0\n",
      "Beginning processing data.\n",
      "Elapsed time: 00:00:00.2134276\n",
      "Finished writing 28481 rows.\n",
      "Writing completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>PredictedLabel</th>\n",
       "      <th>Score.1</th>\n",
       "      <th>Probability.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.623235</td>\n",
       "      <td>1.097949</td>\n",
       "      <td>0.748810</td>\n",
       "      <td>0.763394</td>\n",
       "      <td>-0.179458</td>\n",
       "      <td>-0.258895</td>\n",
       "      <td>0.430106</td>\n",
       "      <td>0.466788</td>\n",
       "      <td>-0.935937</td>\n",
       "      <td>-0.283034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012280</td>\n",
       "      <td>-0.236499</td>\n",
       "      <td>-0.327825</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>0.089418</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.480879</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.155748</td>\n",
       "      <td>-0.998223</td>\n",
       "      <td>-1.158978</td>\n",
       "      <td>-0.992298</td>\n",
       "      <td>-0.484600</td>\n",
       "      <td>-0.308857</td>\n",
       "      <td>-0.677077</td>\n",
       "      <td>-0.193517</td>\n",
       "      <td>-0.083026</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.017962</td>\n",
       "      <td>-0.009465</td>\n",
       "      <td>-0.118435</td>\n",
       "      <td>-0.006350</td>\n",
       "      <td>-0.053375</td>\n",
       "      <td>58.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.389917</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.614893</td>\n",
       "      <td>-0.194953</td>\n",
       "      <td>-2.050402</td>\n",
       "      <td>1.469645</td>\n",
       "      <td>0.540352</td>\n",
       "      <td>-0.665439</td>\n",
       "      <td>0.677713</td>\n",
       "      <td>-0.246032</td>\n",
       "      <td>-0.079937</td>\n",
       "      <td>-0.181429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520915</td>\n",
       "      <td>0.091351</td>\n",
       "      <td>-0.749140</td>\n",
       "      <td>-0.023202</td>\n",
       "      <td>-0.003519</td>\n",
       "      <td>198.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.811233</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.908756</td>\n",
       "      <td>-2.517443</td>\n",
       "      <td>0.277391</td>\n",
       "      <td>-1.466555</td>\n",
       "      <td>-1.521858</td>\n",
       "      <td>3.005920</td>\n",
       "      <td>-2.800770</td>\n",
       "      <td>0.981435</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>1.171678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.905736</td>\n",
       "      <td>-0.511626</td>\n",
       "      <td>0.097492</td>\n",
       "      <td>0.147579</td>\n",
       "      <td>-0.036551</td>\n",
       "      <td>82.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.316100</td>\n",
       "      <td>0.000664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.120853</td>\n",
       "      <td>-1.048240</td>\n",
       "      <td>-1.895990</td>\n",
       "      <td>-1.236063</td>\n",
       "      <td>-0.038722</td>\n",
       "      <td>-0.274832</td>\n",
       "      <td>-0.388942</td>\n",
       "      <td>-0.196979</td>\n",
       "      <td>-0.649028</td>\n",
       "      <td>1.014140</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.322770</td>\n",
       "      <td>0.308495</td>\n",
       "      <td>0.132642</td>\n",
       "      <td>-0.057106</td>\n",
       "      <td>-0.074845</td>\n",
       "      <td>93.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.221130</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.623235  1.097949  0.748810  0.763394 -0.179458 -0.258895  0.430106   \n",
       "1  2.155748 -0.998223 -1.158978 -0.992298 -0.484600 -0.308857 -0.677077   \n",
       "2  1.614893 -0.194953 -2.050402  1.469645  0.540352 -0.665439  0.677713   \n",
       "3  1.908756 -2.517443  0.277391 -1.466555 -1.521858  3.005920 -2.800770   \n",
       "4  2.120853 -1.048240 -1.895990 -1.236063 -0.038722 -0.274832 -0.388942   \n",
       "\n",
       "         V8        V9       V10      ...             V24       V25       V26  \\\n",
       "0  0.466788 -0.935937 -0.283034      ...       -0.012280 -0.236499 -0.327825   \n",
       "1 -0.193517 -0.083026  0.705357      ...       -1.017962 -0.009465 -0.118435   \n",
       "2 -0.246032 -0.079937 -0.181429      ...       -0.520915  0.091351 -0.749140   \n",
       "3  0.981435  0.349534  1.171678      ...       -0.905736 -0.511626  0.097492   \n",
       "4 -0.196979 -0.649028  1.014140      ...       -1.322770  0.308495  0.132642   \n",
       "\n",
       "        V27       V28  Amount  Class  PredictedLabel   Score.1  Probability.1  \n",
       "0  0.023302  0.089418   50.00    0.0               0 -7.480879       0.000563  \n",
       "1 -0.006350 -0.053375   58.00    0.0               0 -8.389917       0.000227  \n",
       "2 -0.023202 -0.003519  198.00    0.0               0 -6.811233       0.001100  \n",
       "3  0.147579 -0.036551   82.00    0.0               0 -7.316100       0.000664  \n",
       "4 -0.057106 -0.074845   93.75    0.0               0 -8.221130       0.000269  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation set to score.\n",
    "logit_predict_df = rx_predict(logit, data=df_val, \n",
    "                           extra_vars_to_write=[TARGET], \n",
    "                           write_model_vars=True)\n",
    "logit_predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 88.235%, Specificity: 99.863%, Threshold: 0.010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Cutoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>28391</td>\n",
       "      <td>6</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.998628</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TP  FP     TN  FN  Sensitivity  Specificity  Cutoff\n",
       "0  45  39  28391   6     0.882353     0.998628    0.01"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Confusion Matrix with Sensitivity and Specificity\n",
    "logit_predictions, logit_stats = get_predict_frame(logit_predict_df, TARGET)\n",
    "logit_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEaCAYAAADtxAsqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCFExiRpCCGEyJgkDSGEEBn7C1WguG6PwFUMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183985b89e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show Receiver Operating Characteristic Curve, AUC\n",
    "get_roc(logit_predictions['Y'].values, logit_predictions['Prob'].values\n",
    "        , 'ROC - Fraud Detection - Validation Set')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
